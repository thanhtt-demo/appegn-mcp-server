{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "98f10973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain>=0.2.16 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-aws>=0.2.2 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (0.2.31)\n",
      "Requirement already satisfied: boto3 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (1.40.25)\n",
      "Requirement already satisfied: requests in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (4.13.3)\n",
      "Requirement already satisfied: psycopg2-binary in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (2.9.10)\n",
      "Requirement already satisfied: tqdm in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: langchain_community in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (0.3.29)\n",
      "Requirement already satisfied: pyvi in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from langchain>=0.2.16) (0.3.75)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from langchain>=0.2.16) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from langchain>=0.2.16) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from langchain>=0.2.16) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from langchain>=0.2.16) (2.0.40)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from langchain>=0.2.16) (6.0.2)\n",
      "Requirement already satisfied: numpy<3,>=1.26.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from langchain-aws>=0.2.2) (2.2.5)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.25 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from boto3) (1.40.25)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from boto3) (0.13.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from beautifulsoup4) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from langchain_community) (3.11.16)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from langchain_community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: sklearn-crfsuite in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from pyvi) (0.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.19.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from botocore<1.41.0,>=1.40.25->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain>=0.2.16) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from langsmith>=0.1.17->langchain>=0.2.16) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from langsmith>=0.1.17->langchain>=0.2.16) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from langsmith>=0.1.17->langchain>=0.2.16) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from langsmith>=0.1.17->langchain>=0.2.16) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.2.16) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.2.16) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.2.16) (0.4.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2.16) (3.1.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.7 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.9.11)\n",
      "Requirement already satisfied: tabulate>=0.4.2 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain>=0.2.16) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain>=0.2.16) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain>=0.2.16) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain>=0.2.16) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.25->boto3) (1.17.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\thanh\\anaconda3\\envs\\python312\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain>=0.2.16) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"langchain>=0.2.16\" \"langchain-aws>=0.2.2\" boto3 requests beautifulsoup4 psycopg2-binary tqdm sentence-transformers langchain_community pyvi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "82c40b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# bedrock_pgvector_demo.py\n",
    "import os, re, json, hashlib, math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import boto3\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # local CPU/GPU\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "\n",
    "WIKI_URL = \"https://vi.wikipedia.org/wiki/Chi%E1%BA%BFn_tranh_Vi%E1%BB%87t_Nam\"\n",
    "PG_HOST = os.getenv(\"PG_HOST\")\n",
    "PG_PORT = os.getenv(\"PG_PORT\")\n",
    "PG_DATABASE = os.getenv(\"PG_DATABASE\")\n",
    "PG_USER = os.getenv(\"PG_USER\")\n",
    "PG_PASSWORD = os.getenv(\"PG_PASSWORD\")\n",
    "REGION = os.getenv(\"AWS_REGION\")\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "# MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# MODEL_NAME = \"dangvantuan/vietnamese-embedding\"\n",
    "MODEL_NAME = \"AITeamVN/Vietnamese_Embedding\"\n",
    "PG_DSN   = f\"postgresql://{PG_USER}:{PG_PASSWORD}@{PG_HOST}:{PG_PORT}/{PG_DATABASE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2fadb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Utils: tải & làm sạch wiki --------\n",
    "def fetch_wiki(url: str) -> Tuple[str, str, str]:\n",
    "    r = requests.get(url, headers={\"User-Agent\": \"RAG-Pgvector-Local\"}, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    title = soup.find(\"h1\", id=\"firstHeading\")\n",
    "    title = title.get_text(strip=True) if title else (soup.title.get_text(strip=True) if soup.title else url)\n",
    "    lang = soup.find(\"html\").get(\"lang\") if soup.find(\"html\") else \"vi\"\n",
    "    content_div = soup.find(\"div\", id=\"mw-content-text\")\n",
    "    text = content_div.get_text(separator=\"\\n\", strip=True) if content_div else soup.get_text(\"\\n\", strip=True)\n",
    "    text = re.sub(r\"\\[\\d+\\]\", \"\", text)      # bỏ [1], [2]...\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text).strip()\n",
    "    return title, lang, text\n",
    "\n",
    "def split_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Hàm cũ: Chia text bằng RecursiveCharacterTextSplitter\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "        chunk_size=1000, chunk_overlap=150, length_function=len\n",
    "    )\n",
    "    chunks = [c.page_content for c in splitter.create_documents([text])]\n",
    "    return [c for c in chunks if len(c) >= 50]\n",
    "\n",
    "def split_text_by_paragraphs(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Hàm mới: Chia text theo từng đoạn văn, đảm bảo mỗi chunk tối đa 500 từ\n",
    "    \"\"\"\n",
    "    def count_words(text: str) -> int:\n",
    "        \"\"\"Đếm số từ trong text tiếng Việt sử dụng pyvi\"\"\"\n",
    "        try:\n",
    "            # tokenized = tokenize(text)\n",
    "            words = text.split()\n",
    "            return len(words)\n",
    "        except:\n",
    "            # Fallback nếu pyvi không hoạt động\n",
    "            return len(text.split())\n",
    "    \n",
    "    # text_send = tokenize(text)\n",
    "    text_send = text\n",
    "\n",
    "    # Chia theo đoạn văn (double newline)\n",
    "    paragraphs = text_send.split('.\\n')\n",
    "\n",
    "    # Lọc bỏ các đoạn quá ngắn và làm sạch\n",
    "    chunks = []\n",
    "    for para in paragraphs:\n",
    "        para = para.strip()\n",
    "        # remove /n\n",
    "        para = para.replace('\\n', ' ')\n",
    "        word_count = count_words(para)\n",
    "        \n",
    "        # Chỉ giữ các đoạn có tối thiểu 10 từ\n",
    "        if word_count < 10:\n",
    "            continue\n",
    "            \n",
    "        # Nếu đoạn có độ dài hợp lý (10-500 từ), thêm trực tiếp\n",
    "        if word_count <= 200:\n",
    "            chunks.append(para)\n",
    "        else:\n",
    "            # Nếu đoạn quá dài (>500 từ), chia nhỏ hơn theo câu\n",
    "            sentences = re.split(r'\\.[\\s]+', para)\n",
    "            current_chunk = \"\"\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                sentence = sentence.strip()\n",
    "                if not sentence:\n",
    "                    continue\n",
    "                    \n",
    "                # Thêm dấu chấm nếu câu chưa có\n",
    "                if not sentence.endswith('.'):\n",
    "                    sentence += '.'\n",
    "                \n",
    "                # Kiểm tra nếu thêm câu này vào chunk hiện tại có vượt quá 500 từ không\n",
    "                test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
    "                test_word_count = count_words(test_chunk)\n",
    "                \n",
    "                if test_word_count <= 200:\n",
    "                    current_chunk = test_chunk\n",
    "                else:\n",
    "                    # Lưu chunk hiện tại nếu đủ dài (tối thiểu 10 từ)\n",
    "                    if count_words(current_chunk) >= 10:\n",
    "                        chunks.append(current_chunk.strip())\n",
    "                    current_chunk = sentence\n",
    "            \n",
    "            # Lưu chunk cuối cùng nếu đủ dài\n",
    "            if count_words(current_chunk) >= 10:\n",
    "                chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# -------- Embeddings (LOCAL) --------\n",
    "def get_embedder():\n",
    "    # HuggingFaceEmbeddings tải model local qua sentence-transformers\n",
    "    return HuggingFaceEmbeddings(model_name=MODEL_NAME, model_kwargs={\"use_auth_token\": HUGGINGFACEHUB_API_TOKEN})\n",
    "\n",
    "def detect_dims(embedder) -> int:\n",
    "    v = embedder.embed_query(\"ping\")\n",
    "    return len(v)\n",
    "\n",
    "def embed_query(embedder, query: str) -> List[float]:\n",
    "    \"\"\"Helper function to embed a single query\"\"\"\n",
    "    return embedder.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b102ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_connect():\n",
    "    return psycopg2.connect(PG_DSN)\n",
    "\n",
    "def upsert_document(conn, url: str, title: str, lang: str, checksum: bytes) -> int:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO rag.document (source_url, title, language, checksum)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "            ON CONFLICT (source_url) DO UPDATE\n",
    "              SET title = EXCLUDED.title, language = EXCLUDED.language, checksum = EXCLUDED.checksum\n",
    "            RETURNING id\n",
    "        \"\"\", (url, title, lang, psycopg2.Binary(checksum)))\n",
    "        doc_id = cur.fetchone()[0]\n",
    "        conn.commit()\n",
    "        return doc_id\n",
    "\n",
    "def insert_chunks(conn, doc_id: int, chunks: List[str], vectors: List[List[float]]):\n",
    "    # Chèn bulk; pgvector literal là chuỗi '[v1,v2,...]'\n",
    "    rows = []\n",
    "    for i, (txt, vec) in enumerate(zip(chunks, vectors)):\n",
    "        vec_str = \"[\" + \",\".join(f\"{x:.7f}\" for x in vec) + \"]\"\n",
    "        rows.append((\n",
    "            doc_id, i, txt, None, len(txt), vec_str\n",
    "        ))\n",
    "    with conn.cursor() as cur:\n",
    "        execute_values(cur, \"\"\"\n",
    "            INSERT INTO rag.chunk (document_id, ordinal, content, n_tokens, char_count, embedding)\n",
    "            VALUES %s\n",
    "        \"\"\", rows)\n",
    "    conn.commit()\n",
    "\n",
    "# ---------- 5) SEARCH ----------\n",
    "def semantic_search(conn, query_vec: List[float], top_k=5):\n",
    "    vec_str = \"[\" + \",\".join(f\"{x:.7f}\" for x in query_vec) + \"]\"\n",
    "    sql = \"\"\"\n",
    "        SELECT id, content,\n",
    "               1 - (embedding <=> %s::vector) AS cosine_similarity\n",
    "        FROM rag.chunk\n",
    "        ORDER BY embedding <=> %s::vector ASC\n",
    "        LIMIT %s\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(sql, (vec_str, vec_str, top_k))\n",
    "        return cur.fetchall()\n",
    "\n",
    "\n",
    "def hybrid_search(conn, query_text: str, query_vec: List[float], top_k=5, rrf_k=60):\n",
    "    \"\"\"\n",
    "    Reciprocal Rank Fusion (RRF): f = 1/(K + rank)\n",
    "    K ~ 60 là con số thực nghiệm phổ biến.\n",
    "    \"\"\"\n",
    "    vec_str = \"[\" + \",\".join(f\"{x:.7f}\" for x in query_vec) + \"]\"\n",
    "    sql = f\"\"\"\n",
    "    WITH sem AS (\n",
    "      SELECT id, content,\n",
    "             ROW_NUMBER() OVER (ORDER BY embedding <=> %s::vector ASC) AS rnk_sem\n",
    "      FROM rag.chunk\n",
    "      LIMIT 200\n",
    "    ),\n",
    "    fts AS (\n",
    "      SELECT id, content,\n",
    "             ROW_NUMBER() OVER (ORDER BY ts_rank_cd(content_tsv, plainto_tsquery('simple', %s)) DESC) AS rnk_fts\n",
    "      FROM rag.chunk\n",
    "      WHERE content_tsv @@ plainto_tsquery('simple', %s)\n",
    "      LIMIT 200\n",
    "    ),\n",
    "    combo AS (\n",
    "      SELECT COALESCE(sem.id, fts.id) AS id,\n",
    "             COALESCE(sem.content, fts.content) AS content,\n",
    "             (CASE WHEN sem.rnk_sem IS NULL THEN 0 ELSE 1.0/(%s + sem.rnk_sem) END) +\n",
    "             (CASE WHEN fts.rnk_fts IS NULL THEN 0 ELSE 1.0/(%s + fts.rnk_fts) END) AS rrf\n",
    "      FROM sem FULL OUTER JOIN fts USING (id)\n",
    "    )\n",
    "    SELECT id, content\n",
    "    FROM combo\n",
    "    ORDER BY rrf DESC\n",
    "    LIMIT %s;\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(sql, (vec_str, query_text, query_text, rrf_k, rrf_k, top_k))\n",
    "        return cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f0e88e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Tải Wikipedia ==\n",
      "Title: Chiến tranh Việt Nam | Lang: vi | Length: 364953 chars\n",
      "\n",
      "== So sánh 2 phương pháp chia chunks ==\n",
      "Chunks (phương pháp cũ - RecursiveCharacterTextSplitter): 458\n",
      "Độ dài chunk đầu tiên: 971 ký tự\n",
      "Preview: Bài này\n",
      "quá dài\n",
      ", khiến việc đọc và tìm kiếm thông tin trở nên khó khăn.\n",
      "Bạn có thể\n",
      "tách\n",
      "nội dung thành các bài nhỏ hơn,\n",
      "cô đọng\n",
      "nội dung lại, và thêm bớt\n",
      "các đề mục con\n",
      ".\n",
      "(\n",
      "tháng 2/2024\n",
      ")\n",
      "Về lịch sử ...\n",
      "\n",
      "Chunks (phương pháp mới - theo đoạn văn): 921\n",
      "Độ dài chunk đầu tiên: 71 ký tự\n",
      "Preview: Bài này quá dài , khiến việc đọc và tìm kiếm thông tin trở nên khó khăn...\n",
      "\n",
      "=> Sử dụng phương pháp mới với 921 chunks\n"
     ]
    }
   ],
   "source": [
    "print(\"== Tải Wikipedia ==\")\n",
    "title, lang, text = fetch_wiki(WIKI_URL)\n",
    "checksum = hashlib.sha256(text.encode(\"utf-8\")).digest()\n",
    "print(f\"Title: {title} | Lang: {lang} | Length: {len(text)} chars\")\n",
    "\n",
    "print(\"\\n== So sánh 2 phương pháp chia chunks ==\")\n",
    "\n",
    "# Phương pháp cũ - RecursiveCharacterTextSplitter\n",
    "chunks_old = split_text(text)\n",
    "print(f\"Chunks (phương pháp cũ - RecursiveCharacterTextSplitter): {len(chunks_old)}\")\n",
    "if chunks_old:\n",
    "    print(f\"Độ dài chunk đầu tiên: {len(chunks_old[0])} ký tự\")\n",
    "    print(f\"Preview: {chunks_old[0][:200]}...\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Phương pháp mới - chia theo đoạn văn\n",
    "chunks_new = split_text_by_paragraphs(text)\n",
    "print(f\"Chunks (phương pháp mới - theo đoạn văn): {len(chunks_new)}\")\n",
    "if chunks_new:\n",
    "    print(f\"Độ dài chunk đầu tiên: {len(chunks_new[0])} ký tự\")\n",
    "    print(f\"Preview: {chunks_new[0][:200]}...\")\n",
    "\n",
    "# Sử dụng phương pháp mới cho các bước tiếp theo\n",
    "chunks = chunks_new\n",
    "print(f\"\\n=> Sử dụng phương pháp mới với {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d61eec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Khởi tạo LOCAL embeddings ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thanh\\anaconda3\\envs\\python312\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:196: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v4 of SentenceTransformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dims: 1024\n",
      "== Tạo vector ==\n",
      "== Lưu document & chunks ==\n"
     ]
    }
   ],
   "source": [
    "print(\"== Khởi tạo LOCAL embeddings ==\")\n",
    "embedder = get_embedder()\n",
    "dims = detect_dims(embedder)   # <- tự nhận kích thước vector\n",
    "print(\"Embedding dims:\", dims)\n",
    "\n",
    "print(\"== Tạo vector ==\")\n",
    "vectors = embedder.embed_documents(chunks)\n",
    "\n",
    "conn = db_connect()\n",
    "print(\"== Lưu document & chunks ==\")\n",
    "doc_id = upsert_document(conn, WIKI_URL, title, lang, checksum)\n",
    "insert_chunks(conn, doc_id, chunks, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "50b2da6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Truy vấn thử (semantic) ==\n",
      "\n",
      "[#387] cos_sim=0.4041\n",
      "Sự cạnh tranh chiến tranh lạnh với Liên Xô là mối quan tâm lớn nhất về chính sách đối ngoại của Mỹ t...\n",
      "\n",
      "[#993] cos_sim=0.3628\n",
      "Among them are: encouragement to national aspirations under non-Communist leadership for peoples of ...\n",
      "\n",
      "[#362] cos_sim=0.3529\n",
      "Sau khi chiến tranh kết thúc, sự chia rẽ Trung-Xô xảy ra kết hợp mâu thuẫn giữa nhà nước Việt Nam th...\n",
      "\n",
      "[#882] cos_sim=0.3439\n",
      "[ 1 ] Các báo cáo của chính phủ Hoa Kỳ hiện cũng trích dẫn ngày 1 tháng 11 năm 1955 là thời điểm bắt...\n",
      "\n",
      "[#440] cos_sim=0.2239\n",
      "Đồng thời với việc từ chối tuyển cử, chế độ Việt Nam Cộng hòa ra sức củng cố quyền lực, đàn áp khốc ...\n",
      "\n",
      "[#482] cos_sim=0.1975\n",
      "Quốc hội Việt Nam Dân chủ Cộng hòa trong Nghị quyết ngày 20 tháng 9 năm 1955 tuyên bố: Dựa trên bản ...\n",
      "\n",
      "[#364] cos_sim=0.1846\n",
      "[ 71 ] Một nguồn khác thống kê rằng tổng lượng chất nổ mà quân đội Hoa Kỳ sử dụng trong chiến tranh ...\n",
      "\n",
      "[#1256] cos_sim=0.1587\n",
      "Summons of Trumpet: U.S.–Vietnam in Perspective . Novato, CA: Presidio Press...\n",
      "\n",
      "[#1106] cos_sim=0.1378\n",
      "Bản gốc lưu trữ ngày 23 tháng 2 năm 2013 . Truy cập ngày 16 tháng 2 năm 2013...\n",
      "\n",
      "[#957] cos_sim=0.1224\n",
      "Le Génocide Khmer Rouge: Une Analyse Démographique [ The Khmer Rouge genocide: A demographic analysi...\n",
      "\n",
      "== Truy vấn thử (hybrid RRF) ==\n",
      "\n",
      "[#358] Chiến tranh Việt Nam Một phần của Chiến tranh Đông Dương và Chiến tranh Lạnh Theo chiều kim đồng hồ ...\n",
      "\n",
      "[#1025] ^ Daniel Ellsberg trong cuốn Secrets: A Memoir of Vietnam and the Pentagon Papers, Viking, 2002, p.2...\n",
      "\n",
      "[#1179] ^ Nguồn từ Bảo tàng Chứng tích Chiến tranh - 28 Võ Văn Tần Q.3 Thành phố Hồ Chí Minh ^ Những nguồn c...\n",
      "\n",
      "[#387] Sự cạnh tranh chiến tranh lạnh với Liên Xô là mối quan tâm lớn nhất về chính sách đối ngoại của Mỹ t...\n",
      "\n",
      "[#385] Hơn nữa, ngay từ năm 1949 , sau khi Nội chiến Trung Quốc kết thúc, tiếp đó là chiến tranh Triều Tiên...\n",
      "\n",
      "[#1270] Liên kết ngoài Wikimedia Commons có thêm hình ảnh và phương tiện về Chiến tranh Việt Nam...\n",
      "\n",
      "[#1072] ^ George C. Herring. Cuộc chiến dài ngày giữa nước Mỹ và Việt Nam 1950 - 1975. Nhà xuất bản Công an ...\n",
      "\n",
      "[#1109] Lỗi sfn: không có mục tiêu: CITEREFCục_Tâm_Lý_Chiến1965 ( trợ giúp ) ^ Ellsberg, Daniel, Secrets: A ...\n",
      "\n",
      "[#1274] Ford Tổng thống VNCH: Ngô Đình Diệm Bất ổn Nguyễn Văn Thiệu Trần Văn Hương VNDCCH - Tổng Bí thư: Lê ...\n",
      "\n",
      "[#1211] Những nguồn viện trợ cho Cách mạng VN từ các nước Xã hội Chủ nghĩa Lưu trữ ngày 15 tháng 4 năm 2005 ...\n"
     ]
    }
   ],
   "source": [
    "print(\"== Truy vấn thử (semantic) ==\")\n",
    "conn = db_connect()\n",
    "q = \"Mối liên hệ giữa chiến tranh Việt Nam và chiến tranh Đông Phương ?\"\n",
    "# q_next = tokenize(q)\n",
    "# print(\"Query:\", q_next)\n",
    "qvec = embed_query(embedder, q)\n",
    "rows = semantic_search(conn, qvec, top_k=10)\n",
    "for rid, content, sim in rows:\n",
    "    print(f\"\\n[#{rid}] cos_sim={sim:.4f}\\n{content[:100]}...\")\n",
    "\n",
    "print(\"\\n== Truy vấn thử (hybrid RRF) ==\")\n",
    "rows = hybrid_search(conn, q, qvec, top_k=10)\n",
    "for rid, content in rows:\n",
    "    print(f\"\\n[#{rid}] {content[:100]}...\")\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
